{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4fghS8MnOaH",
        "outputId": "7819184d-dc82-4414-9936-824d1ea63ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'irp-db24'...\n",
            "remote: Enumerating objects: 616, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 616 (delta 94), reused 56 (delta 37), pack-reused 441 (from 1)\u001b[K\n",
            "Receiving objects: 100% (616/616), 235.55 MiB | 19.63 MiB/s, done.\n",
            "Resolving deltas: 100% (248/248), done.\n",
            "/content/irp-db24\n",
            "Updating files: 100% (130/130), done.\n",
            "Branch 'Modeldesigntransformer' set up to track remote branch 'Modeldesigntransformer' from 'origin'.\n",
            "Switched to a new branch 'Modeldesigntransformer'\n"
          ]
        }
      ],
      "source": [
        "## git clone\n",
        "\n",
        "# Use your token securely (don't share!)\n",
        "\n",
        "token = \"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\"\n",
        "!git clone https://{token}@github.com/ese-ada-lovelace-2024/irp-db24.git\n",
        "\n",
        "# # Add to Python path\n",
        "import sys\n",
        "sys.path.append('/content/irp-db24/')\n",
        "\n",
        "# Step 1: Go into the cloned repo directory\n",
        "%cd /content/irp-db24\n",
        "\n",
        "# Step 2: Fetch all remote branches (just to be safe)\n",
        "!git fetch\n",
        "\n",
        "# Step 3: Checkout the Modeldesigntransformer branch\n",
        "!git checkout Modeldesigntransformer\n",
        "\n",
        "# # Now import as usual\n",
        "from Modules.data_loader import SlidingWindowDataset, PatientLatentDataset\n",
        "from Modules.model import PositionalEncoding, CustomTransformerEncoderLayer, VariationalTimeSeriesTransformer, LatentDiscriminator\n",
        "from Modules.one_step.train import train_model_aae_onestep\n",
        "from Modules.utils import extract_latents_by_condition, predict_with_model, normalize_risk_vector, get_risk_from_prediction, get_baseline_prediction_vector, scan_individual_risk, compute_reactivity_score, get_baseline_vector, scan_risk\n",
        "from Modules.visuals import evaluate_model_minute, evaluate_model_hourly, plot_attention_across_layers, plot_attention_across_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpVblcl6Qoum"
      },
      "outputs": [],
      "source": [
        "## imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bh7Rvu345y6",
        "outputId": "db7e4118-12b3-4c1d-c18e-3aea3538804e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1swb68BNKkTLpjAFhFoUGBBOLazNbf91w\n",
            "From (redirected): https://drive.google.com/uc?id=1swb68BNKkTLpjAFhFoUGBBOLazNbf91w&confirm=t&uuid=beb8e775-8832-4c0b-a600-58a2c995c4ff\n",
            "To: /content/irp-db24/train_data_not_normalised.csv\n",
            "100% 129M/129M [00:01<00:00, 127MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15mAxxKukregwjmhMA8nw_1Pz-p5YkwtZ\n",
            "To: /content/irp-db24/test_data_not_normalised.csv\n",
            "100% 67.5M/67.5M [00:00<00:00, 70.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "# Use the file ID\n",
        "!gdown --id 1swb68BNKkTLpjAFhFoUGBBOLazNbf91w --output train_data_not_normalised.csv\n",
        "!gdown --id 15mAxxKukregwjmhMA8nw_1Pz-p5YkwtZ --output test_data_not_normalised.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYF62xMT0WBk",
        "outputId": "3a7b37d8-ab3e-4dcd-afca-f6323a5d6ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data has unique patient of [['INH001' 'INH004' 'INH007' 'INH010' 'INH011' 'INH012' 'INH015' 'INH127'\n",
            " 'INH018' 'INH023' 'INH024' 'INH028' 'INH100' 'INH101' 'INH102' 'INH107'\n",
            " 'INH114' 'INH108' 'INH109' 'INH110' 'INH112' 'INH115' 'INH119' 'INH120'\n",
            " 'INH121' 'INH123' 'INH128' 'INH131' 'INH138' 'INH139']]\n",
            "Testing Data has unique patient of [['INH002' 'INH003' 'INH006' 'INH013' 'INH014' 'INH017' 'INH025' 'INH027'\n",
            " 'INH103' 'INH106' 'INH113' 'INH117' 'INH135']]\n",
            "Validation Data has unique patient of [['INH111']]\n",
            "326894 159025 12738\n",
            "30 13 1\n"
          ]
        }
      ],
      "source": [
        "test_data_import = pd.read_csv(\"test_data_not_normalised.csv\")\n",
        "train_data_nn_pre = pd.read_csv(\"train_data_not_normalised.csv\")\n",
        "val_data_nn_pre = test_data_import[test_data_import[\"patient_id\"] == 'INH111'].copy()\n",
        "test_data_nn_pre = test_data_import[test_data_import[\"patient_id\"] != 'INH111'].copy()\n",
        "\n",
        "print(f\"Training Data has unique patient of [{train_data_nn_pre['patient_id'].unique()}]\")\n",
        "print(f\"Testing Data has unique patient of [{test_data_nn_pre['patient_id'].unique()}]\")\n",
        "print(f\"Validation Data has unique patient of [{val_data_nn_pre['patient_id'].unique()}]\")\n",
        "print(len(train_data_nn_pre), len(test_data_nn_pre), len(val_data_nn_pre))\n",
        "print(len(train_data_nn_pre[\"patient_id\"].unique()), len(test_data_nn_pre[\"patient_id\"].unique()), len(val_data_nn_pre[\"patient_id\"].unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5JrVAIK08yp"
      },
      "outputs": [],
      "source": [
        "train_data_nn = train_data_nn_pre.copy()\n",
        "test_data_nn = test_data_nn_pre.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoJYME4xRYs3"
      },
      "outputs": [],
      "source": [
        "# # # ## Useful to filter down!\n",
        "\n",
        "# # # # Training Data has unique patient of [['INH001' 'INH004' 'INH007' 'INH010' 'INH011' 'INH012' 'INH015' 'INH127'\n",
        "# # # #  'INH018' 'INH023' 'INH024' 'INH028' 'INH100' 'INH101' 'INH102' 'INH107'\n",
        "# # # #  'INH114' 'INH108' 'INH109' 'INH110' 'INH112' 'INH115' 'INH119' 'INH120'\n",
        "# # # #  'INH121' 'INH123' 'INH128' 'INH131' 'INH138' 'INH139']]\n",
        "# # # # Testing Data has unique patient of [['INH002' 'INH003' 'INH006' 'INH013' 'INH014' 'INH017' 'INH025' 'INH027'\n",
        "# # # #  'INH103' 'INH106' 'INH113' 'INH117' 'INH135']]\n",
        "\n",
        "# train_data_nn_pre = train_data_nn_pre[train_data_nn_pre[\"patient_id\"].isin(['INH001','INH004', 'INH007', 'INH102' ,'INH107' ,'INH114'])]\n",
        "# test_data_nn_pre = test_data_nn_pre[test_data_nn_pre[\"patient_id\"].isin(['INH002','INH014','INH017' ])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0kh4zrUIc-d"
      },
      "outputs": [],
      "source": [
        "## ================= INSTANTIATING AND CALLING THE MODEL ==================\n",
        "# Hyperparameters\n",
        "window_size = 8\n",
        "forecast_steps = 240\n",
        "batch_size = 64\n",
        "d_model = 128\n",
        "nhead = 8\n",
        "num_layers = 6\n",
        "dim_feedforward = 512\n",
        "dropout = 0.2\n",
        "lr = 1e-3\n",
        "epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data_train = train_data_nn_pre.copy()\n",
        "data_test = test_data_nn_pre.copy()\n",
        "\n",
        "feature_cols = ['br_avg', 'br_std', 'act_level', 'step_count', 'pm2_5_x', 'temperature', 'humidity', 'hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'yearly_sin',\n",
        "       'yearly_cos', 'lat_round', 'lon_round','pm10', 'no', 'no2', 'o3', 'so2', 'co','inhale_tv']\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data_train[feature_cols])\n",
        "\n",
        "train_data = scaler.transform(data_train[feature_cols])\n",
        "test_data = scaler.transform(data_test[feature_cols])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SlidingWindowDataset(train_data, window_size, forecast_steps)\n",
        "test_dataset = SlidingWindowDataset(test_data, window_size, forecast_steps)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = VariationalTimeSeriesTransformer(\n",
        "    input_features=len(feature_cols),\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_layers=num_layers,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    dropout=dropout,\n",
        "    forecast_steps=forecast_steps,\n",
        "    output_features=len(feature_cols)\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "discriminator = LatentDiscriminator(latent_dim=model.latent_dim).to(device)\n",
        "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "\n",
        "# Train\n",
        "train_model_aae_onestep(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    discriminator=discriminator,\n",
        "    disc_optimizer=disc_optimizer,\n",
        "    device=device,\n",
        "    epochs=epochs,\n",
        "    adv_weight=0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_sU1GN8ITsu"
      },
      "source": [
        "## Plotting Model Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJ_FRRSP1Lb"
      },
      "outputs": [],
      "source": [
        "## Plotting model results - minute level!\n",
        "\n",
        "feature_names = feature_cols\n",
        "# Evaluate and plot\n",
        "preds, trues = evaluate_model_minute(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    device=device,\n",
        "    scaler=scaler,\n",
        "    feature_names=feature_names\n",
        ")\n",
        "\n",
        "# Additional metrics calculation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"MSE: {mean_squared_error(trues, preds):.4f}\")\n",
        "print(f\"MAE: {mean_absolute_error(trues, preds):.4f}\")\n",
        "print(f\"R² Score: {r2_score(trues, preds):.4f}\")\n",
        "\n",
        "# Per-feature metrics\n",
        "print(\"\\nPer-feature Metrics:\")\n",
        "for i, feature in enumerate(feature_names):\n",
        "    mse = mean_squared_error(trues[:, i], preds[:, i])\n",
        "    mae = mean_absolute_error(trues[:, i], preds[:, i])\n",
        "    r2 = r2_score(trues[:, i], preds[:, i])\n",
        "\n",
        "    print(f\"{feature}:\")\n",
        "    print(f\"  MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w418FWQbP1OA"
      },
      "outputs": [],
      "source": [
        "## Evaluation and Plot hourly\n",
        "\n",
        "# Plotting model results - hourly level\n",
        "\n",
        "# Run hourly evaluation\n",
        "preds_hourly, trues_hourly = evaluate_model_hourly(\n",
        "    model=model,\n",
        "    dataloader=test_loader,\n",
        "    device=device,\n",
        "    scaler=scaler,\n",
        "    feature_names=feature_cols\n",
        ")\n",
        "\n",
        "# Hourly-level metrics\n",
        "print(\"\\nHourly Aggregated Metrics:\")\n",
        "print(f\"MSE: {mean_squared_error(trues_hourly, preds_hourly):.4f}\")\n",
        "print(f\"MAE: {mean_absolute_error(trues_hourly, preds_hourly):.4f}\")\n",
        "print(f\"R² Score: {r2_score(trues_hourly, preds_hourly):.4f}\")\n",
        "\n",
        "# Per-feature metrics\n",
        "print(\"\\nPer-feature Hourly Metrics:\")\n",
        "for i, feature in enumerate(feature_cols):\n",
        "    mse = mean_squared_error(trues_hourly[:, i], preds_hourly[:, i])\n",
        "    mae = mean_absolute_error(trues_hourly[:, i], preds_hourly[:, i])\n",
        "    r2 = r2_score(trues_hourly[:, i], preds_hourly[:, i])\n",
        "    print(f\"{feature}:\")\n",
        "    print(f\"  MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLCH6gkx7Ycq"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3hPm7Et2Ohi5"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
