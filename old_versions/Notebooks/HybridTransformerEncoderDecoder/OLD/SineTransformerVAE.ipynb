{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbEflzVkNHw6iZvzxAIYRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ese-ada-lovelace-2024/irp-db24/blob/Modeldesigntransformer/SineTransformerVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au3mjudFM1e8",
        "outputId": "c5def3aa-c6d9-4fc0-afdb-2e2bfec671e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cpVblcl6Qoum"
      },
      "outputs": [],
      "source": [
        "## imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define time vector (e.g., from 0 to 10 seconds, with 1000 points)\n",
        "time = np.linspace(0, 86400, 25_000)  # 100k points\n",
        "frequency = 1 / 86400   # e.g., 3 full sine cycles per day  # Reasonable daily frequency\n",
        "amplitude = 2.3547  # unitless (change this to adjust the wave amplitude)\n",
        "\n",
        "# # Generate sine wave values\n",
        "# sine_wave = amplitude * np.sin(2 * np.pi * frequency * time)\n",
        "\n",
        "# Add noise to make learning more robust\n",
        "sine_wave = amplitude * np.sin(2 * np.pi * frequency * time) + np.random.normal(0, 0.1, len(time))\n",
        "\n",
        "# Create DataFrame\n",
        "sine_df_raw = pd.DataFrame({\n",
        "    'Time (seconds)': time,\n",
        "    'Sine Amplitude': sine_wave\n",
        "})\n",
        "\n",
        "# Total seconds in a day and week\n",
        "SECONDS_PER_HOUR = 3600\n",
        "SECONDS_PER_DAY = 86400\n",
        "SECONDS_PER_WEEK = SECONDS_PER_DAY * 7\n",
        "\n",
        "# Compute cyclical features\n",
        "sine_df_raw['dow_cos'] = np.cos(2 * np.pi * (sine_df_raw['Time (seconds)'] % SECONDS_PER_WEEK) / SECONDS_PER_WEEK)\n",
        "sine_df_raw['dow_sin'] = np.sin(2 * np.pi * (sine_df_raw['Time (seconds)'] % SECONDS_PER_WEEK) / SECONDS_PER_WEEK)\n",
        "sine_df_raw['hour_cos'] = np.cos(2 * np.pi * (sine_df_raw['Time (seconds)'] % SECONDS_PER_DAY) / SECONDS_PER_DAY)\n",
        "\n",
        "print(sine_df_raw)\n",
        "sine_df = sine_df_raw.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uuEffJOyy8K",
        "outputId": "9badb4de-17db-46e3-d236-d748bc6bd7bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Time (seconds)  Sine Amplitude   dow_cos   dow_sin  hour_cos\n",
            "0            0.000000        0.002239  1.000000  0.000000  1.000000\n",
            "1            3.456138        0.146615  1.000000  0.000036  1.000000\n",
            "2            6.912276       -0.173893  1.000000  0.000072  1.000000\n",
            "3           10.368415        0.238368  1.000000  0.000108  1.000000\n",
            "4           13.824553       -0.013441  1.000000  0.000144  0.999999\n",
            "...               ...             ...       ...       ...       ...\n",
            "24995    86386.175447        0.064090  0.623602  0.781742  0.999999\n",
            "24996    86389.631585       -0.057861  0.623574  0.781764  1.000000\n",
            "24997    86393.087724        0.001201  0.623546  0.781787  1.000000\n",
            "24998    86396.543862       -0.021340  0.623518  0.781809  1.000000\n",
            "24999    86400.000000       -0.007447  0.623490  0.781831  1.000000\n",
            "\n",
            "[25000 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Before passing the data to the model, I need to drop time features\n",
        "sine_df = sine_df.drop(columns=['Time (seconds)'])\n",
        "sine_df = (sine_df - sine_df.mean()) / sine_df.std()"
      ],
      "metadata": {
        "id": "mz3SNQ7OyzmE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# === Positional Encoding for continuous features ===\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "# === Transformer VAE with Dual-Head Decoder ===\n",
        "class TransformerVAE(nn.Module):\n",
        "    def __init__(self, feature_dim=4, d_model=64, latent_dim=16, num_heads=4, num_layers=2, dropout=0.1, dim_feedforward = 256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_mu = nn.Linear(d_model, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(d_model, latent_dim)\n",
        "\n",
        "        self.latent_to_hidden = nn.Linear(latent_dim, d_model)\n",
        "\n",
        "        # Separate decoder layers for reconstruction and forecasting\n",
        "        decoder_layer_recon = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        decoder_layer_forecast = nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "\n",
        "        self.transformer_decoder_recon = nn.TransformerDecoder(decoder_layer_recon, num_layers=num_layers)\n",
        "        self.transformer_decoder_forecast = nn.TransformerDecoder(decoder_layer_forecast, num_layers=num_layers)\n",
        "\n",
        "        self.output_proj_recon = nn.Linear(d_model, feature_dim)\n",
        "        self.output_proj_forecast = nn.Linear(d_model, feature_dim)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        encoded = self.transformer_encoder(x)\n",
        "        pooled = encoded.mean(dim=0)\n",
        "        mu = self.fc_mu(pooled)\n",
        "        logvar = self.fc_logvar(pooled)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode_reconstruct(self, z, seq_len, tgt_input):\n",
        "        hidden = self.latent_to_hidden(z).unsqueeze(0)\n",
        "        output = self.transformer_decoder_recon(tgt_input, memory=hidden)\n",
        "        output = self.output_proj_recon(output.transpose(0, 1))\n",
        "        return output\n",
        "\n",
        "    def decode_forecast(self, z, forecast_len):\n",
        "        hidden = self.latent_to_hidden(z).unsqueeze(0)\n",
        "        tgt_input = torch.zeros(forecast_len, hidden.size(1), hidden.size(2), device=z.device)\n",
        "        output = self.transformer_decoder_forecast(tgt_input, memory=hidden)\n",
        "        output = self.output_proj_forecast(output.transpose(0, 1))\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, forecast_len=1):\n",
        "        seq_len = x.size(1)\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        x_input = self.input_proj(x)\n",
        "        x_input = self.pos_encoder(x_input)\n",
        "        x_input = x_input.transpose(0, 1)\n",
        "\n",
        "        recon = self.decode_reconstruct(z, seq_len, tgt_input=x_input)\n",
        "        forecast = self.decode_forecast(z, forecast_len)\n",
        "\n",
        "        return recon, forecast, mu, logvar\n"
      ],
      "metadata": {
        "id": "9ZcgmNk1y3RW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "forecast_steps = 1 # Number of steps to forecast\n",
        "seq_len = 500  # Length of input sequence\n",
        "num_features = 4\n",
        "\n",
        "# Adjust total timesteps to accommodate forecast target\n",
        "data_tensor = torch.tensor(sine_df.values, dtype=torch.float32)\n",
        "print(data_tensor.shape)\n",
        "total_timesteps = data_tensor.shape[0] - (data_tensor.shape[0] % (seq_len + forecast_steps))\n",
        "print(total_timesteps)\n",
        "data_tensor = data_tensor[:total_timesteps]\n",
        "\n",
        "# Reshape with extended sequence length\n",
        "sequences = data_tensor.view(-1, seq_len + forecast_steps, num_features)\n",
        "\n",
        "# Split sequences into input and target\n",
        "input_seq = sequences[:, :seq_len, :]         # [num_samples, seq_len, num_features]\n",
        "future_target = sequences[:, seq_len:, :]     # [num_samples, forecast_steps, num_features]\n",
        "\n",
        "# Train/Val split\n",
        "split_idx = int(0.8 * len(sequences))\n",
        "train_dataset = TensorDataset(input_seq[:split_idx], future_target[:split_idx])\n",
        "val_dataset = TensorDataset(input_seq[split_idx:], future_target[split_idx:])\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ5xyLvay4sI",
        "outputId": "c8e5ea7c-d14a-4242-e117-560ad71c06a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25000, 4])\n",
            "24549\n",
            "Train batches: 2, Val batches: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not overfitting anymore but the validation phase is not working as great, so I will add:\n",
        "-"
      ],
      "metadata": {
        "id": "OA_bNIIzLwe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = TransformerVAE(\n",
        "    feature_dim=4,\n",
        "    d_model=64,\n",
        "    latent_dim=64,\n",
        "    num_heads=4,\n",
        "    num_layers=3,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
        "recon_loss_fn = nn.MSELoss()\n",
        "forecast_loss_fn = nn.MSELoss()\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "\n",
        "for epoch in range(31):\n",
        "    vae.train()\n",
        "    epoch_loss, recon_loss_total, forecast_loss_total = 0, 0, 0\n",
        "\n",
        "    for x, future_target in train_loader:\n",
        "\n",
        "        recon, forecast, mu, logvar = vae(x, forecast_len=1)\n",
        "\n",
        "        recon_loss = recon_loss_fn(recon, x)\n",
        "        forecast_loss = forecast_loss_fn(forecast, future_target)\n",
        "        loss = recon_loss + forecast_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(vae.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        recon_loss_total += recon_loss.item()\n",
        "        forecast_loss_total += forecast_loss.item()\n",
        "\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    avg_recon = recon_loss_total / len(train_loader)\n",
        "    avg_forecast = forecast_loss_total / len(train_loader)\n",
        "\n",
        "\n",
        "    # ðŸ”¥ Validation Phase\n",
        "    vae.eval()\n",
        "    val_recon_total, val_forecast_total, val_kld_total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, future_target_val in val_loader:\n",
        "            recon_val, forecast_val, mu_val, logvar_val = vae(x_val, forecast_len=1)\n",
        "\n",
        "            val_recon_loss = recon_loss_fn(recon_val, x_val)\n",
        "            val_forecast_loss = forecast_loss_fn(forecast_val, future_target_val)\n",
        "            val_kld = -0.5 * torch.mean(1 + logvar_val - mu_val.pow(2) - logvar_val.exp())\n",
        "\n",
        "            val_recon_total += val_recon_loss.item()\n",
        "            val_forecast_total += val_forecast_loss.item()\n",
        "            val_kld_total += val_kld.item()\n",
        "\n",
        "    avg_val_recon = val_recon_total / len(val_loader)\n",
        "    avg_val_forecast = val_forecast_total / len(val_loader)\n",
        "    avg_val_kld = val_kld_total / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss {avg_loss:.4f} (Recon {avg_recon:.4f}, Forecast {avg_forecast:.4f} | Val (Recon {avg_val_recon:.4f}, Forecast {avg_val_forecast:.4f})\")\n",
        "\n",
        "    results.append({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": avg_loss,\n",
        "        \"train_recon_loss\": avg_recon,\n",
        "        \"train_forecast_loss\": avg_forecast,\n",
        "        \"val_recon_loss\": avg_val_recon,\n",
        "        \"val_forecast_loss\": avg_val_forecast,\n",
        "        \"val_kld_loss\": avg_val_kld\n",
        "    })\n",
        "\n",
        "# Save results\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"vae_dualdecoder_trainval.csv\", index=False)\n",
        "print(\"Results saved successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6v1gUZMy6tq",
        "outputId": "8bdd75bf-09c6-422a-cc39-c033b7acd44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 2, Val batches: 1\n",
            "Epoch 1: Train Loss 2.0737 (Recon 1.0733, Forecast 1.0004 | Val (Recon 1.9323, Forecast 3.8370)\n",
            "Epoch 2: Train Loss 1.0085 (Recon 0.3979, Forecast 0.6106 | Val (Recon 1.1893, Forecast 1.4493)\n",
            "Epoch 3: Train Loss 0.5493 (Recon 0.1881, Forecast 0.3611 | Val (Recon 1.3236, Forecast 1.4007)\n",
            "Epoch 4: Train Loss 0.4670 (Recon 0.1491, Forecast 0.3178 | Val (Recon 0.9622, Forecast 1.3081)\n",
            "Epoch 5: Train Loss 0.3179 (Recon 0.1191, Forecast 0.1988 | Val (Recon 0.7544, Forecast 1.3077)\n",
            "Epoch 6: Train Loss 0.2769 (Recon 0.0936, Forecast 0.1833 | Val (Recon 0.6938, Forecast 1.1008)\n",
            "Epoch 7: Train Loss 0.2283 (Recon 0.0938, Forecast 0.1345 | Val (Recon 0.6141, Forecast 0.9229)\n",
            "Epoch 8: Train Loss 0.2061 (Recon 0.0690, Forecast 0.1371 | Val (Recon 0.7327, Forecast 0.7930)\n",
            "Epoch 9: Train Loss 0.1706 (Recon 0.0745, Forecast 0.0961 | Val (Recon 0.7721, Forecast 0.8147)\n",
            "Epoch 10: Train Loss 0.1610 (Recon 0.0589, Forecast 0.1021 | Val (Recon 0.6064, Forecast 0.9104)\n",
            "Epoch 11: Train Loss 0.1548 (Recon 0.0495, Forecast 0.1053 | Val (Recon 0.4652, Forecast 0.8984)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_forecast_amplitude(model, x_init, steps=30, amplitude_idx=0):\n",
        "    \"\"\"\n",
        "    Predict multiple future steps recursively using model's forecast head.\n",
        "\n",
        "    Args:\n",
        "        model: trained TransformerVAE model\n",
        "        x_init: [batch_size, seq_len, feature_dim] - starting sequence\n",
        "        steps: int - future steps to predict\n",
        "        amplitude_idx: int - index of amplitude feature to extract\n",
        "\n",
        "    Returns:\n",
        "        preds: [batch_size, steps] - predicted amplitude values\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    x_curr = x_init.clone().detach()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(steps):\n",
        "            recon, future_pred, _, _ = model(x_curr, forecast_len=1)  # Predict 1 step\n",
        "            next_feat = future_pred[:, -1, :]  # Next predicted feature vector\n",
        "            next_amp = next_feat[:, amplitude_idx]\n",
        "\n",
        "            preds.append(next_amp.unsqueeze(1))\n",
        "            x_curr = torch.cat([x_curr[:, 1:, :], next_feat.unsqueeze(1)], dim=1)\n",
        "\n",
        "    preds = torch.cat(preds, dim=1)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "miGf43AYy8UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all = 500\n",
        "\n",
        "amplitude_idx = sine_df.columns.get_loc('Sine Amplitude')\n",
        "\n",
        "# Example sequence for context\n",
        "example_seq = sequences[0].unsqueeze(0)  # [1, seq_len, feature_dim]\n",
        "\n",
        "# True future amplitude for comparison\n",
        "true_amplitude = data_tensor[seq_len : seq_len + all, amplitude_idx]\n",
        "\n",
        "# Predict recursively\n",
        "predicted_amplitude = recursive_forecast_amplitude(vae, example_seq, steps=all, amplitude_idx=amplitude_idx)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(range(all), true_amplitude.cpu(), label='True Amplitude', marker='o', markersize=3)\n",
        "plt.plot(range(all), predicted_amplitude.squeeze(0).cpu(), label='Predicted Amplitude', marker='x', markersize=3)\n",
        "plt.title(f\"Recursive Sine Wave Amplitude Prediction ({all} Steps Ahead)\")\n",
        "plt.xlabel(\"Future Timestep\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJEQEnsgy83Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-rvYAqI2IqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}